{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# start my homework\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import cv2 # Used to manipulated the images \n",
    "np.random.seed(1337)\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input\n",
    "from keras.layers import concatenate as keras_concat\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_json('J:/kaggle/iceberg/train.json') #read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaled_imgs(df): #reshape the data and create a new image channel\n",
    "    imgs = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        #make 75x75 image\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 - band_2\n",
    "\n",
    "        # Rescale\n",
    "        a = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n",
    "        b = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n",
    "        c = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n",
    "\n",
    "        imgs.append(np.dstack((a, b, c)))\n",
    "\n",
    "    return np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = get_scaled_imgs(df_train) \n",
    "Ytrain = np.array(df_train['is_iceberg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train only with (inc_angle not NA)\n",
    "idx_tr = np.where(df_train.inc_angle!=\"na\")\n",
    "Ytrain = Ytrain[idx_tr[0]]\n",
    "Xtrain = Xtrain[idx_tr[0],...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_more_images(imgs): #flip, increase samples\n",
    "    \n",
    "    more_images = []\n",
    "    vert_flip_imgs = []\n",
    "    hori_flip_imgs = []\n",
    "      \n",
    "    for i in range(0,imgs.shape[0]):\n",
    "        a=imgs[i,:,:,0]\n",
    "        b=imgs[i,:,:,1]\n",
    "        c=imgs[i,:,:,2]\n",
    "        \n",
    "        av=cv2.flip(a,1)\n",
    "        ah=cv2.flip(a,0)\n",
    "        bv=cv2.flip(b,1)\n",
    "        bh=cv2.flip(b,0)\n",
    "        cv=cv2.flip(c,1)\n",
    "        ch=cv2.flip(c,0)\n",
    "        \n",
    "        vert_flip_imgs.append(np.dstack((av, bv, cv)))\n",
    "        hori_flip_imgs.append(np.dstack((ah, bh, ch)))\n",
    "      \n",
    "    v = np.array(vert_flip_imgs)\n",
    "    h = np.array(hori_flip_imgs)\n",
    "       \n",
    "    more_images = np.concatenate((imgs,v,h))\n",
    "    \n",
    "    return more_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_more = get_more_images(Xtrain)\n",
    "Ytr_more = np.concatenate((Ytrain,Ytrain,Ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel_inception():\n",
    "    #google incepyion with residual learning\n",
    "    input_img = Input(shape=(75, 75, 3))\n",
    "    tower_1 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img)\n",
    "    tower_1 = Conv2D(64, (3, 3), padding='same', activation='relu')(tower_1)\n",
    "\n",
    "    tower_2 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img)\n",
    "    tower_2 = Conv2D(64, (5, 5), padding='same', activation='relu')(tower_2)\n",
    "\n",
    "    tower_3 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_img)\n",
    "    tower_3 = Conv2D(64, (1, 1), padding='same', activation='relu')(tower_3)\n",
    "\n",
    "    x = keras_concat([input_img, tower_1, tower_2, tower_3], axis=-1)\n",
    "\n",
    "    #add Conv and MaxPooling layers to reduce parameters\n",
    "    x = Conv2D(64, (3, 3), padding='valid', activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), padding='valid', activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), padding='valid', activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # You must flatten the data for the dense layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    # output\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=input_img, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(lr=0.001,decay=0.0),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 75, 75, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 75, 75, 64)   256         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 75, 75, 64)   256         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 75, 75, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 75, 75, 64)   36928       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 75, 75, 64)   102464      conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 75, 75, 64)   256         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 75, 75, 195)  0           input_1[0][0]                    \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 73, 73, 64)   112384      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 36, 36, 64)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 34, 34, 64)   36928       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 64)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 15, 15, 64)   36928       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 7, 7, 64)     0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3136)         0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          803072      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            129         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,162,497\n",
      "Trainable params: 1,162,497\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# fit getModel_inception to be an example\n",
    "model = getModel_inception()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('model_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3309 samples, validate on 1104 samples\n",
      "Epoch 1/50\n",
      "3309/3309 [==============================] - 831s 251ms/step - loss: 0.5517 - acc: 0.6969 - val_loss: 0.4110 - val_acc: 0.8025\n",
      "Epoch 2/50\n",
      "3309/3309 [==============================] - 815s 246ms/step - loss: 0.3597 - acc: 0.8447 - val_loss: 0.5139 - val_acc: 0.7690\n",
      "Epoch 3/50\n",
      "3309/3309 [==============================] - 813s 246ms/step - loss: 0.2997 - acc: 0.8631 - val_loss: 0.3086 - val_acc: 0.8569\n",
      "Epoch 4/50\n",
      "3309/3309 [==============================] - 813s 246ms/step - loss: 0.2561 - acc: 0.8852 - val_loss: 0.2853 - val_acc: 0.8750\n",
      "Epoch 5/50\n",
      "3309/3309 [==============================] - 813s 246ms/step - loss: 0.2269 - acc: 0.9000 - val_loss: 0.2617 - val_acc: 0.8850\n",
      "Epoch 6/50\n",
      "3309/3309 [==============================] - 814s 246ms/step - loss: 0.1841 - acc: 0.9217 - val_loss: 0.2570 - val_acc: 0.8976\n",
      "Epoch 7/50\n",
      "3309/3309 [==============================] - 812s 245ms/step - loss: 0.1870 - acc: 0.9238 - val_loss: 0.2518 - val_acc: 0.8958\n",
      "Epoch 8/50\n",
      "3309/3309 [==============================] - 812s 245ms/step - loss: 0.1477 - acc: 0.9383 - val_loss: 0.2886 - val_acc: 0.8940\n",
      "Epoch 9/50\n",
      "3309/3309 [==============================] - 813s 246ms/step - loss: 0.1268 - acc: 0.9468 - val_loss: 0.2990 - val_acc: 0.8714\n",
      "Epoch 10/50\n",
      "3309/3309 [==============================] - 813s 246ms/step - loss: 0.1173 - acc: 0.9562 - val_loss: 0.2971 - val_acc: 0.9049\n",
      "Epoch 11/50\n",
      "3309/3309 [==============================] - 823s 249ms/step - loss: 0.0989 - acc: 0.9610 - val_loss: 0.3126 - val_acc: 0.9067\n",
      "Epoch 12/50\n",
      "3309/3309 [==============================] - 813s 246ms/step - loss: 0.0635 - acc: 0.9770 - val_loss: 0.5381 - val_acc: 0.8370\n",
      "Epoch 13/50\n",
      "3309/3309 [==============================] - 813s 246ms/step - loss: 0.0539 - acc: 0.9804 - val_loss: 0.5151 - val_acc: 0.8967\n",
      "Epoch 14/50\n",
      "3309/3309 [==============================] - 811s 245ms/step - loss: 0.0493 - acc: 0.9804 - val_loss: 0.5046 - val_acc: 0.8859\n",
      "Epoch 15/50\n",
      "3296/3309 [============================>.] - ETA: 2s - loss: 0.0475 - acc: 0.9827\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "3309/3309 [==============================] - 811s 245ms/step - loss: 0.0480 - acc: 0.9825 - val_loss: 0.4012 - val_acc: 0.8804\n",
      "Epoch 16/50\n",
      "3309/3309 [==============================] - 813s 246ms/step - loss: 0.0271 - acc: 0.9924 - val_loss: 0.4017 - val_acc: 0.9049\n",
      "Epoch 17/50\n",
      "3309/3309 [==============================] - 813s 246ms/step - loss: 0.0099 - acc: 0.9976 - val_loss: 0.4283 - val_acc: 0.9013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xd835d30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtr_more, Ytr_more, batch_size=batch_size, epochs=50, verbose=1, callbacks=[earlyStopping, mcp_save, reduce_lr_loss], validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath = 'model_wts.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1471/1471 [==============================] - 109s 74ms/step\n",
      "Train score: 0.13245033653596078\n",
      "Train accuracy: 0.9462950373895309\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(Xtrain, Ytrain, verbose=1)\n",
    "print('Train score:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_json('J:/kaggle/iceberg/test.json')\n",
    "Xtest = (get_scaled_imgs(df_test))\n",
    "pred_test = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  is_iceberg\n",
      "0  5941774d    0.366425\n",
      "1  4023181e    0.904179\n",
      "2  b20200e4    0.025609\n",
      "3  e7f018bb    0.995582\n",
      "4  4371c8c3    0.039017\n",
      "5  a8d9b1fd    0.001022\n",
      "6  29e7727e    0.054846\n",
      "7  92a51ffb    0.996019\n",
      "8  c769ac97    0.000066\n",
      "9  aee0547d    0.000002\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame({'id': df_test[\"id\"], 'is_iceberg': pred_test.reshape((pred_test.shape[0]))})\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save submission\n",
    "submission.to_csv('J:/kaggle/iceberg/submission.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
